{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsiQdD8tKxig"
      },
      "source": [
        "\n",
        "# Text Generation with Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "5VR-QMpsMY_w"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8COJN9MKxij"
      },
      "source": [
        "## Functions for Processing Text\n",
        "\n",
        "### Reading in files as a string text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoKSb4vhLY8O",
        "outputId": "6fd2b40d-dd4c-4522-aa07-8f36322bfa61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "Q2rieHBEKxik"
      },
      "outputs": [],
      "source": [
        "def read_file(filepath):\n",
        "    with open(filepath) as f:\n",
        "        str_text = f.read().lower()\n",
        "    return str_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {
        "id": "CvUqloMSKxil"
      },
      "outputs": [],
      "source": [
        "text = read_file('/content/drive/MyDrive/hiparis/moby_dick_four_chapters.txt')\n",
        "text = text.replace(\"\\n\", \" \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[:600]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "jlVVYMazL7dA",
        "outputId": "df1041f7-4857-4bae-d2ad-be6528f41c1d"
      },
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'call me ishmael.  some years ago--never mind how long precisely--having little or no money in my purse, and nothing particular to interest me on shore, i thought i would sail about a little and see the watery part of the world.  it is a way i have of driving off the spleen and regulating the circulation.  whenever i find myself growing grim about the mouth; whenever it is a damp, drizzly november in my soul; whenever i find myself involuntarily pausing before coffin warehouses, and bringing up the rear of every funeral i meet; and especially whenever my hypos get such an upper hand of me, that'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co61QE7FKxil"
      },
      "source": [
        "### Tokenize and Clean Text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the 4th chapter of moby dick, the vocabulary of this character based text generation model\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVub99w-Nb_p",
        "outputId": "0b68ae29-1724-4175-c33c-9a3127c5d296"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "id": "TZH1QtJeKxim"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "-SW9baA2QBZw"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "TAXPEYHfQzSb"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "seq_length = 10\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVDseNPuR19-",
        "outputId": "ab46d563-8de5-4bbd-9cbe-f20b4c146db3"
      },
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([b'c' b'a' b'l' b'l' b' ' b'm' b'e' b' ' b'i' b's' b'h'], shape=(11,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBIkhGKOSReM",
        "outputId": "8d183268-ce62-4160-8094-701cf3835252"
      },
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'call me ish'\n",
            "b'mael.  some'\n",
            "b' years ago-'\n",
            "b'-never mind'\n",
            "b' how long p'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "7TPgZnPYT1WM"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzI5DBD_UAOn",
        "outputId": "599a240a-f002-49c4-dc97-0a071f7d9db5"
      },
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'call me is'\n",
            "Target: b'all me ish'\n",
            "<_MapDataset element_spec=(TensorSpec(shape=(10,), dtype=tf.int64, name=None), TensorSpec(shape=(10,), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training batches"
      ],
      "metadata": {
        "id": "83WbYjOrUUDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N8TI5pjUTmk",
        "outputId": "71fae325-8d69-4507-bb62-b29602491ec7"
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(32, 10), dtype=tf.int64, name=None), TensorSpec(shape=(32, 10), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umv3KTnWKxip"
      },
      "source": [
        "# Creating an LSTM based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {
        "id": "58f_ijeFKxip"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {
        "id": "NZq3pGjPKxip"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.LSTM(rnn_units, return_sequences=True)\n",
        "    self.lstm1 = tf.keras.layers.LSTM(rnn_units, return_sequences=True)\n",
        "    self.lstm2 = tf.keras.layers.LSTM(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.lstm.get_initial_state(x)\n",
        "    x = self.lstm(x, initial_state=states, training=training)\n",
        "    x = self.lstm1(x, training=training)\n",
        "    x, mem_states, car_states = self.lstm2(x, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, mem_states, car_states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVHuZ7puUw-V",
        "outputId": "96440cef-43b9-47d9-fbd8-fdd19349e0dc"
      },
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 10, 41) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"my_model_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_27 (Embedding)    multiple                  10496     \n",
            "                                                                 \n",
            " lstm_49 (LSTM)              multiple                  1574912   \n",
            "                                                                 \n",
            " lstm_50 (LSTM)              multiple                  2099200   \n",
            "                                                                 \n",
            " lstm_51 (LSTM)              multiple                  2099200   \n",
            "                                                                 \n",
            " dense_27 (Dense)            multiple                  21033     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,804,841\n",
            "Trainable params: 5,804,841\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLvVFoAJKxiq"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyXoNGrcKxiq",
        "outputId": "4b2f9ce5-2351-42a8-ba4e-c5f2b2dc6be5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "175/175 - 7s - loss: 2.8800 - 7s/epoch - 38ms/step\n",
            "Epoch 2/30\n",
            "175/175 - 1s - loss: 2.3910 - 1s/epoch - 7ms/step\n",
            "Epoch 3/30\n",
            "175/175 - 1s - loss: 2.1721 - 1s/epoch - 8ms/step\n",
            "Epoch 4/30\n",
            "175/175 - 2s - loss: 2.0250 - 2s/epoch - 9ms/step\n",
            "Epoch 5/30\n",
            "175/175 - 1s - loss: 1.9065 - 1s/epoch - 7ms/step\n",
            "Epoch 6/30\n",
            "175/175 - 1s - loss: 1.8144 - 1s/epoch - 7ms/step\n",
            "Epoch 7/30\n",
            "175/175 - 1s - loss: 1.7275 - 1s/epoch - 7ms/step\n",
            "Epoch 8/30\n",
            "175/175 - 1s - loss: 1.6444 - 1s/epoch - 7ms/step\n",
            "Epoch 9/30\n",
            "175/175 - 1s - loss: 1.5702 - 1s/epoch - 7ms/step\n",
            "Epoch 10/30\n",
            "175/175 - 1s - loss: 1.4911 - 1s/epoch - 7ms/step\n",
            "Epoch 11/30\n",
            "175/175 - 1s - loss: 1.4104 - 1s/epoch - 7ms/step\n",
            "Epoch 12/30\n",
            "175/175 - 1s - loss: 1.3313 - 1s/epoch - 7ms/step\n",
            "Epoch 13/30\n",
            "175/175 - 2s - loss: 1.2462 - 2s/epoch - 9ms/step\n",
            "Epoch 14/30\n",
            "175/175 - 1s - loss: 1.1654 - 1s/epoch - 7ms/step\n",
            "Epoch 15/30\n",
            "175/175 - 1s - loss: 1.0903 - 1s/epoch - 7ms/step\n",
            "Epoch 16/30\n",
            "175/175 - 1s - loss: 1.0159 - 1s/epoch - 7ms/step\n",
            "Epoch 17/30\n",
            "175/175 - 1s - loss: 0.9478 - 1s/epoch - 8ms/step\n",
            "Epoch 18/30\n",
            "175/175 - 1s - loss: 0.8923 - 1s/epoch - 8ms/step\n",
            "Epoch 19/30\n",
            "175/175 - 1s - loss: 0.8482 - 1s/epoch - 8ms/step\n",
            "Epoch 20/30\n",
            "175/175 - 1s - loss: 0.8093 - 1s/epoch - 8ms/step\n",
            "Epoch 21/30\n",
            "175/175 - 1s - loss: 0.7818 - 1s/epoch - 9ms/step\n",
            "Epoch 22/30\n",
            "175/175 - 1s - loss: 0.7613 - 1s/epoch - 8ms/step\n",
            "Epoch 23/30\n",
            "175/175 - 1s - loss: 0.7410 - 1s/epoch - 7ms/step\n",
            "Epoch 24/30\n",
            "175/175 - 1s - loss: 0.7283 - 1s/epoch - 7ms/step\n",
            "Epoch 25/30\n",
            "175/175 - 1s - loss: 0.7169 - 1s/epoch - 8ms/step\n",
            "Epoch 26/30\n",
            "175/175 - 1s - loss: 0.7093 - 1s/epoch - 7ms/step\n",
            "Epoch 27/30\n",
            "175/175 - 1s - loss: 0.6987 - 1s/epoch - 7ms/step\n",
            "Epoch 28/30\n",
            "175/175 - 1s - loss: 0.6938 - 1s/epoch - 8ms/step\n",
            "Epoch 29/30\n",
            "175/175 - 1s - loss: 0.6869 - 1s/epoch - 7ms/step\n",
            "Epoch 30/30\n",
            "175/175 - 1s - loss: 0.6833 - 1s/epoch - 7ms/step\n"
          ]
        }
      ],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "EPOCHS = 30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback], verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKFx8ee0Kxiq"
      },
      "source": [
        "# Generating New Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "id": "hqQvk0ojKxiq"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=0.6):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    predicted_logits, mem_states, carry_states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "id": "H7KmKGnFKxiq"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['The sea is full of'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlVCLi2cZbHU",
        "outputId": "010c928e-7653-41ca-ed3e-1dc467f193f4"
      },
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sea is full of be olind we d be soo the thaind t tou t t haroolat tht blind ind the heres hanthes osu t goule thin \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 0.2990429401397705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results are not really good, that's probably due to the fact that we don't have much data. Moreover, a character-based text generation RNN model needs more data to be accurate"
      ],
      "metadata": {
        "id": "4lbrGnI690Oe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try to build a model unsig pre trained GPT\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DkL1jgh93t2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q keras-nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok2sFmX-3tZb",
        "outputId": "d98d6912-b8e6-4bd3-80b8-43c932f18b2b"
      },
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.7/527.7 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_nlp\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "bN8uG5UL3yDE"
      },
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=128,\n",
        ")\n",
        "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "    \"gpt2_base_en\", preprocessor=preprocessor\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J5vCm-23-eA",
        "outputId": "aeeb4cfc-113e-4020-fab6-4b8ae78a9e5a"
      },
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/vocab.json\n",
            "1042301/1042301 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/merges.txt\n",
            "456318/456318 [==============================] - 0s 1us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-nlp/models/gpt2_base_en/v1/model.h5\n",
            "497986112/497986112 [==============================] - 14s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'token_embedding/embeddings:0' shape=(50257, 768) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "output = gpt2_lm.generate(\"My interview at Institut Polytechnique de Paris went\", max_length=50)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPGKnuDy3-93",
        "outputId": "0ba20474-770f-4d58-b66c-5821ae3128e4"
      },
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "My interview at Institut Polytechnique de Paris went well and I was able to get a better understanding of the project, which will help me better understand how this project works. I am happy to say that the project will be a good fit\n",
            "TOTAL TIME ELAPSED: 16.34s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = read_file('/content/drive/MyDrive/hiparis/moby_dick_four_chapters.txt')\n",
        "text = text.replace(\"\\n\", \" \")\n",
        "text = text.split(\".\")\n",
        "print(text[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq1_H2wM6Zeb",
        "outputId": "49554a77-23ae-4c27-d86c-08acbf3791ff"
      },
      "execution_count": 338,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call me ishmael\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(text)\n",
        "    .batch(64)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
        "    5e-4,\n",
        "    decay_steps=train_ds.cardinality() * num_epochs,\n",
        "    end_learning_rate=0.0,\n",
        ")\n",
        "\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "gpt2_lm.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "gpt2_lm.summary()\n",
        "\n",
        "gpt2_lm.fit(train_ds, epochs=num_epochs, verbose = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg5Qj9G24FaN",
        "outputId": "500c8b91-d9cb-452b-8661-db8d84e5e36b"
      },
      "execution_count": 339,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessor: \"gpt2_causal_lm_preprocessor\"\n",
            "__________________________________________________________________________________________________\n",
            " Tokenizer (type)                                    Vocab #     \n",
            "==================================================================================================\n",
            " gpt2_tokenizer (GPT2Tokenizer)                      50257       \n",
            "__________________________________________________________________________________________________\n",
            "                                                                                                  \n",
            "Model: \"gpt2_causal_lm\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " padding_mask (InputLayer)      [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " token_ids (InputLayer)         [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " gpt2_backbone (GPT2Backbone)   (None, None, 768)    124439808   ['padding_mask[0][0]',           \n",
            "                                                                  'token_ids[0][0]']              \n",
            "                                                                                                  \n",
            " tf.linalg.matmul (TFOpLambda)  (None, None, 50257)  0           ['gpt2_backbone[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 124,439,808\n",
            "Trainable params: 124,439,808\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "7/7 - 99s - loss: 0.4721 - accuracy: 0.6672 - 99s/epoch - 14s/step\n",
            "Epoch 2/10\n",
            "7/7 - 1s - loss: 0.3109 - accuracy: 0.7617 - 948ms/epoch - 135ms/step\n",
            "Epoch 3/10\n",
            "7/7 - 1s - loss: 0.2133 - accuracy: 0.8312 - 921ms/epoch - 132ms/step\n",
            "Epoch 4/10\n",
            "7/7 - 1s - loss: 0.1562 - accuracy: 0.8727 - 915ms/epoch - 131ms/step\n",
            "Epoch 5/10\n",
            "7/7 - 1s - loss: 0.1190 - accuracy: 0.9025 - 951ms/epoch - 136ms/step\n",
            "Epoch 6/10\n",
            "7/7 - 1s - loss: 0.0987 - accuracy: 0.9223 - 938ms/epoch - 134ms/step\n",
            "Epoch 7/10\n",
            "7/7 - 1s - loss: 0.0876 - accuracy: 0.9304 - 938ms/epoch - 134ms/step\n",
            "Epoch 8/10\n",
            "7/7 - 1s - loss: 0.0799 - accuracy: 0.9369 - 926ms/epoch - 132ms/step\n",
            "Epoch 9/10\n",
            "7/7 - 1s - loss: 0.0765 - accuracy: 0.9413 - 948ms/epoch - 135ms/step\n",
            "Epoch 10/10\n",
            "7/7 - 1s - loss: 0.0744 - accuracy: 0.9430 - 929ms/epoch - 133ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9629c39d80>"
            ]
          },
          "metadata": {},
          "execution_count": 339
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = gpt2_lm.generate(\"The sea\", max_length=200)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohD5v2bB5zxX",
        "outputId": "49bc630f-53f4-418f-ffda-56380002f9eb"
      },
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sea-gods had ordained that he should soon become my shipmate (though i cannot tell why it was exactly that those stage managers, the fates, put me down for this shabby part of a whaling voyage, when others were set down for magnificent parts in high tragedies, and short and easy parts in genteel comedies, and jolly parts in farces--though i cannot tell why this was exactly; yet, now that i recall all the circumstances, i think i can see a little into the springs and motives which being cunningly presented to me under various disguises, induced me to set about performing the part i did, besides cajoling me into the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are really good and the text is significantly influenced by the chapter 4 of Moby dick used to fine tune GPT. "
      ],
      "metadata": {
        "id": "VBYJSDWpBBEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further improvements\n",
        "\n",
        "A word based model could be implemented to be compared with the character-based model. \n",
        "Transfer Learning seems to be anyway the best method to achieve good results in NLP tasks"
      ],
      "metadata": {
        "id": "yTB34yzpBObT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gOgqHGdHBg9c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}